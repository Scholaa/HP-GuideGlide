{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ad52f072",
   "metadata": {},
   "source": [
    "DATA PRE-PROCESSING AND SPLITTING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3330a14b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['userId', 'attractionId', 'interactionType', 'timestamp',\n",
      "       'userId_encoded', 'attractionId_encoded', 'interactionType_view',\n",
      "       'user_interaction_count', 'attraction_popularity',\n",
      "       'interaction_recency', 'day_of_week', 'hour_of_day'],\n",
      "      dtype='object')\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "((800, 7), (200, 7), (800,), (200,))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "import boto3\n",
    "\n",
    "# Load the dataset from S3\n",
    "s3 = boto3.client('s3')\n",
    "bucket_name = 'interactionsdataset'\n",
    "file_key = 'synthetic_user_interactions.csv'\n",
    "\n",
    "s3.download_file(bucket_name, file_key, '/tmp/synthetic_user_interactions.csv')\n",
    "df = pd.read_csv('/tmp/synthetic_user_interactions.csv')\n",
    "\n",
    "# Data preprocessing\n",
    "df['timestamp'] = pd.to_datetime(df['timestamp'])\n",
    "label_encoder_user = LabelEncoder()\n",
    "label_encoder_attraction = LabelEncoder()\n",
    "df['userId_encoded'] = label_encoder_user.fit_transform(df['userId'])\n",
    "df['attractionId_encoded'] = label_encoder_attraction.fit_transform(df['attractionId'])\n",
    "\n",
    "# One-hot encode interactionType\n",
    "one_hot_encoder = OneHotEncoder(sparse_output=False)\n",
    "interaction_type_encoded = one_hot_encoder.fit_transform(df[['interactionType']])\n",
    "interaction_type_encoded_df = pd.DataFrame(interaction_type_encoded, columns=one_hot_encoder.get_feature_names_out(['interactionType']))\n",
    "df = pd.concat([df, interaction_type_encoded_df], axis=1)\n",
    "\n",
    "# Feature engineering\n",
    "df['user_interaction_count'] = df.groupby('userId_encoded')['userId_encoded'].transform('count')\n",
    "df['attraction_popularity'] = df.groupby('attractionId_encoded')['attractionId_encoded'].transform('count')\n",
    "max_timestamp = df['timestamp'].max()\n",
    "df['interaction_recency'] = (max_timestamp - df['timestamp']).dt.days\n",
    "df['day_of_week'] = df['timestamp'].dt.dayofweek\n",
    "df['hour_of_day'] = df['timestamp'].dt.hour\n",
    "\n",
    "# Select features for the model\n",
    "features = ['userId_encoded', 'attractionId_encoded', 'user_interaction_count', \n",
    "            'attraction_popularity', 'interaction_recency', 'day_of_week', 'hour_of_day']\n",
    "\n",
    "# Target variable\n",
    "target = 'interactionType_view'\n",
    "\n",
    "# Verify the columns\n",
    "print(df.columns)\n",
    "\n",
    "# Prepare training and testing data\n",
    "X = df[features]\n",
    "y = df[target]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Display the shapes of the resulting datasets\n",
    "X_train.shape, X_test.shape, y_train.shape, y_test.shape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4b82346",
   "metadata": {},
   "source": [
    "UPLOADING PRE-PROCESSED DATA TO THE S3 BUCKET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "03e54fd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "\n",
    "s3 = boto3.client('s3')\n",
    "bucket_name = 'interactionsdataset'\n",
    "prefix = 'sagemaker/collaborative-filtering'\n",
    "\n",
    "# Upload training data to S3\n",
    "train_file = 'train_data.csv'\n",
    "test_file = 'test_data.csv'\n",
    "df_train = pd.concat([X_train, y_train], axis=1)\n",
    "df_test = pd.concat([X_test, y_test], axis=1)\n",
    "df_train.to_csv(f'/tmp/{train_file}', index=False)\n",
    "df_test.to_csv(f'/tmp/{test_file}', index=False)\n",
    "s3.upload_file(f'/tmp/{train_file}', bucket_name, f'{prefix}/{train_file}')\n",
    "s3.upload_file(f'/tmp/{test_file}', bucket_name, f'{prefix}/{test_file}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd8cc552",
   "metadata": {},
   "source": [
    "PREPARING TESTING AND TRAINING DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0619538b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker.amazon.common as smac\n",
    "import boto3\n",
    "import numpy as np\n",
    "import io\n",
    "\n",
    "# Prepare the training data\n",
    "train_features = X_train.to_numpy().astype('float32')\n",
    "train_labels = y_train.to_numpy().astype('float32').flatten()  # Ensure labels are a 1D array\n",
    "\n",
    "# Convert to RecordIO Protobuf format\n",
    "buf = io.BytesIO()\n",
    "smac.write_numpy_to_dense_tensor(buf, train_features, train_labels)\n",
    "buf.seek(0)\n",
    "\n",
    "# Upload the training data to S3\n",
    "bucket = 'interactionsdataset'\n",
    "prefix = 'sagemaker/collaborative-filtering'\n",
    "key = 'train_data.protobuf'\n",
    "s3_object = f'{prefix}/{key}'\n",
    "boto3.resource('s3').Bucket(bucket).Object(s3_object).upload_fileobj(buf)\n",
    "\n",
    "# Prepare the testing data\n",
    "test_features = X_test.to_numpy().astype('float32')\n",
    "test_labels = y_test.to_numpy().astype('float32').flatten()  # Ensure labels are a 1D array\n",
    "\n",
    "# Convert to RecordIO Protobuf format\n",
    "buf = io.BytesIO()\n",
    "smac.write_numpy_to_dense_tensor(buf, test_features, test_labels)\n",
    "buf.seek(0)\n",
    "\n",
    "# Upload the testing data to S3\n",
    "test_key = 'test_data.protobuf'\n",
    "test_s3_object = f'{prefix}/{test_key}'\n",
    "boto3.resource('s3').Bucket(bucket).Object(test_s3_object).upload_fileobj(buf)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddcd54db",
   "metadata": {},
   "source": [
    "TRAINING THE MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c01dd77b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker.image_uris:Same images used for training and inference. Defaulting to image scope: inference.\n",
      "INFO:sagemaker.image_uris:Defaulting to the only supported framework/algorithm version: 1.\n",
      "INFO:sagemaker.image_uris:Ignoring unnecessary instance type: None.\n",
      "INFO:sagemaker:Creating training-job with name: factorization-machines-2024-07-28-12-38-13-255\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-07-28 12:38:13 Starting - Starting the training job...\n",
      "2024-07-28 12:38:28 Starting - Preparing the instances for training...\n",
      "2024-07-28 12:38:55 Downloading - Downloading input data...\n",
      "2024-07-28 12:39:25 Downloading - Downloading the training image.........\n",
      "2024-07-28 12:41:07 Training - Training image download completed. Training in progress....\n",
      "2024-07-28 12:41:37 Uploading - Uploading generated training model\u001b[34mDocker entrypoint called with argument(s): train\u001b[0m\n",
      "\u001b[34mRunning default environment configuration script\u001b[0m\n",
      "\u001b[34m/opt/amazon/lib/python3.8/site-packages/mxnet/model.py:97: SyntaxWarning: \"is\" with a literal. Did you mean \"==\"?\n",
      "  if num_device is 1 and 'dist' not in kvstore:\u001b[0m\n",
      "\u001b[34m[07/28/2024 12:41:30 INFO 140663685162816] Reading default configuration from /opt/amazon/lib/python3.8/site-packages/algorithm/resources/default-conf.json: {'epochs': 1, 'mini_batch_size': '1000', 'use_bias': 'true', 'use_linear': 'true', 'bias_lr': '0.1', 'linear_lr': '0.001', 'factors_lr': '0.0001', 'bias_wd': '0.01', 'linear_wd': '0.001', 'factors_wd': '0.00001', 'bias_init_method': 'normal', 'bias_init_sigma': '0.01', 'linear_init_method': 'normal', 'linear_init_sigma': '0.01', 'factors_init_method': 'normal', 'factors_init_sigma': '0.001', 'batch_metrics_publish_interval': '500', '_data_format': 'record', '_kvstore': 'auto', '_learning_rate': '1.0', '_log_level': 'info', '_num_gpus': 'auto', '_num_kv_servers': 'auto', '_optimizer': 'adam', '_tuning_objective_metric': '', '_use_full_symbolic': 'true', '_wd': '1.0'}\u001b[0m\n",
      "\u001b[34m[07/28/2024 12:41:30 INFO 140663685162816] Merging with provided configuration from /opt/ml/input/config/hyperparameters.json: {'feature_dim': '7', 'mini_batch_size': '100', 'num_factors': '10', 'predictor_type': 'binary_classifier'}\u001b[0m\n",
      "\u001b[34m[07/28/2024 12:41:30 INFO 140663685162816] Final configuration: {'epochs': 1, 'mini_batch_size': '100', 'use_bias': 'true', 'use_linear': 'true', 'bias_lr': '0.1', 'linear_lr': '0.001', 'factors_lr': '0.0001', 'bias_wd': '0.01', 'linear_wd': '0.001', 'factors_wd': '0.00001', 'bias_init_method': 'normal', 'bias_init_sigma': '0.01', 'linear_init_method': 'normal', 'linear_init_sigma': '0.01', 'factors_init_method': 'normal', 'factors_init_sigma': '0.001', 'batch_metrics_publish_interval': '500', '_data_format': 'record', '_kvstore': 'auto', '_learning_rate': '1.0', '_log_level': 'info', '_num_gpus': 'auto', '_num_kv_servers': 'auto', '_optimizer': 'adam', '_tuning_objective_metric': '', '_use_full_symbolic': 'true', '_wd': '1.0', 'feature_dim': '7', 'num_factors': '10', 'predictor_type': 'binary_classifier'}\u001b[0m\n",
      "\u001b[34m[07/28/2024 12:41:30 WARNING 140663685162816] Loggers have already been setup.\u001b[0m\n",
      "\u001b[34mProcess 7 is a worker.\u001b[0m\n",
      "\u001b[34m[07/28/2024 12:41:30 INFO 140663685162816] Using default worker.\u001b[0m\n",
      "\u001b[34m[07/28/2024 12:41:30 INFO 140663685162816] Checkpoint loading and saving are disabled.\u001b[0m\n",
      "\u001b[34m[2024-07-28 12:41:30.665] [tensorio] [warning] TensorIO is already initialized; ignoring the initialization routine.\u001b[0m\n",
      "\u001b[34m[2024-07-28 12:41:30.665] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 0, \"duration\": 1, \"num_examples\": 1, \"num_bytes\": 7200}\u001b[0m\n",
      "\u001b[34m/opt/amazon/python3.8/lib/python3.8/subprocess.py:848: RuntimeWarning: line buffering (buffering=1) isn't supported in binary mode, the default buffer size will be used\n",
      "  self.stdout = io.open(c2pread, 'rb', bufsize)\u001b[0m\n",
      "\u001b[34m[07/28/2024 12:41:30 INFO 140663685162816] nvidia-smi: took 0.030 seconds to run.\u001b[0m\n",
      "\u001b[34m[07/28/2024 12:41:30 INFO 140663685162816] nvidia-smi identified 0 GPUs.\u001b[0m\n",
      "\u001b[34m[07/28/2024 12:41:30 INFO 140663685162816] Number of GPUs being used: 0\u001b[0m\n",
      "\u001b[34m[07/28/2024 12:41:30 INFO 140663685162816] [Fully symbolic network] Building a fully symbolic network.\u001b[0m\n",
      "\u001b[34m[07/28/2024 12:41:30 INFO 140663685162816] Create Store: local\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1722170490.6636713, \"EndTime\": 1722170490.698934, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"initialize.time\": {\"sum\": 33.35881233215332, \"count\": 1, \"min\": 33.35881233215332, \"max\": 33.35881233215332}}}\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1722170490.699055, \"EndTime\": 1722170490.6990838, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"Meta\": \"init_train_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 100.0, \"count\": 1, \"min\": 100, \"max\": 100}, \"Total Batches Seen\": {\"sum\": 1.0, \"count\": 1, \"min\": 1, \"max\": 1}, \"Max Records Seen Between Resets\": {\"sum\": 100.0, \"count\": 1, \"min\": 100, \"max\": 100}, \"Max Batches Seen Between Resets\": {\"sum\": 1.0, \"count\": 1, \"min\": 1, \"max\": 1}, \"Reset Count\": {\"sum\": 1.0, \"count\": 1, \"min\": 1, \"max\": 1}, \"Number of Records Since Last Reset\": {\"sum\": 0.0, \"count\": 1, \"min\": 0, \"max\": 0}, \"Number of Batches Since Last Reset\": {\"sum\": 0.0, \"count\": 1, \"min\": 0, \"max\": 0}}}\u001b[0m\n",
      "\u001b[34m[07/28/2024 12:41:30 INFO 140663685162816] #quality_metric: host=algo-1, epoch=0, batch=0 train binary_classification_accuracy <score>=1.0\u001b[0m\n",
      "\u001b[34m[07/28/2024 12:41:30 INFO 140663685162816] #quality_metric: host=algo-1, epoch=0, batch=0 train binary_classification_cross_entropy <loss>=0.0790973424911499\u001b[0m\n",
      "\u001b[34m[07/28/2024 12:41:30 INFO 140663685162816] #quality_metric: host=algo-1, epoch=0, batch=0 train binary_f_1.000 <score>=1.0\u001b[0m\n",
      "\u001b[34m[2024-07-28 12:41:30.731] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 2, \"duration\": 21, \"num_examples\": 8, \"num_bytes\": 57600}\u001b[0m\n",
      "\u001b[34m[07/28/2024 12:41:30 INFO 140663685162816] Epoch[0] Train-binary_classification_accuracy=1.000000\u001b[0m\n",
      "\u001b[34m[07/28/2024 12:41:30 INFO 140663685162816] Epoch[0] Train-binary_classification_cross_entropy=0.009887\u001b[0m\n",
      "\u001b[34m[07/28/2024 12:41:30 INFO 140663685162816] Epoch[0] Train-binary_f_1.000=1.000000\u001b[0m\n",
      "\u001b[34m[07/28/2024 12:41:30 INFO 140663685162816] Epoch[0] Time cost=0.027\u001b[0m\n",
      "\u001b[34m[07/28/2024 12:41:30 INFO 140663685162816] #quality_metric: host=algo-1, epoch=0, train binary_classification_accuracy <score>=1.0\u001b[0m\n",
      "\u001b[34m[07/28/2024 12:41:30 INFO 140663685162816] #quality_metric: host=algo-1, epoch=0, train binary_classification_cross_entropy <loss>=0.009887167811393737\u001b[0m\n",
      "\u001b[34m[07/28/2024 12:41:30 INFO 140663685162816] #quality_metric: host=algo-1, epoch=0, train binary_f_1.000 <score>=1.0\u001b[0m\n",
      "\u001b[34m[07/28/2024 12:41:30 INFO 140663685162816] #quality_metric: host=algo-1, train binary_classification_accuracy <score>=1.0\u001b[0m\n",
      "\u001b[34m[07/28/2024 12:41:30 INFO 140663685162816] #quality_metric: host=algo-1, train binary_classification_cross_entropy <loss>=0.009887167811393737\u001b[0m\n",
      "\u001b[34m[07/28/2024 12:41:30 INFO 140663685162816] #quality_metric: host=algo-1, train binary_f_1.000 <score>=1.0\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1722170490.699004, \"EndTime\": 1722170490.733536, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"epochs\": {\"sum\": 1.0, \"count\": 1, \"min\": 1, \"max\": 1}, \"update.time\": {\"sum\": 34.23309326171875, \"count\": 1, \"min\": 34.23309326171875, \"max\": 34.23309326171875}}}\u001b[0m\n",
      "\u001b[34m[07/28/2024 12:41:30 INFO 140663685162816] #progress_metric: host=algo-1, completed 100.0 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1722170490.6992786, \"EndTime\": 1722170490.7337248, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 0, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 900.0, \"count\": 1, \"min\": 900, \"max\": 900}, \"Total Batches Seen\": {\"sum\": 9.0, \"count\": 1, \"min\": 9, \"max\": 9}, \"Max Records Seen Between Resets\": {\"sum\": 800.0, \"count\": 1, \"min\": 800, \"max\": 800}, \"Max Batches Seen Between Resets\": {\"sum\": 8.0, \"count\": 1, \"min\": 8, \"max\": 8}, \"Reset Count\": {\"sum\": 3.0, \"count\": 1, \"min\": 3, \"max\": 3}, \"Number of Records Since Last Reset\": {\"sum\": 0.0, \"count\": 1, \"min\": 0, \"max\": 0}, \"Number of Batches Since Last Reset\": {\"sum\": 0.0, \"count\": 1, \"min\": 0, \"max\": 0}}}\u001b[0m\n",
      "\u001b[34m[07/28/2024 12:41:30 INFO 140663685162816] #throughput_metric: host=algo-1, train throughput=23154.241393349297 records/second\u001b[0m\n",
      "\u001b[34m[07/28/2024 12:41:30 WARNING 140663685162816] wait_for_all_workers will not sync workers since the kv store is not running distributed\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1722170490.7336047, \"EndTime\": 1722170490.7340102, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"finalize.time\": {\"sum\": 0.023365020751953125, \"count\": 1, \"min\": 0.023365020751953125, \"max\": 0.023365020751953125}}}\u001b[0m\n",
      "\u001b[34m[07/28/2024 12:41:30 INFO 140663685162816] Saved checkpoint to \"/tmp/tmpwnozby7b/state-0001.params\"\u001b[0m\n",
      "\u001b[34m[2024-07-28 12:41:30.735] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/test\", \"epoch\": 0, \"duration\": 70, \"num_examples\": 1, \"num_bytes\": 7200}\u001b[0m\n",
      "\u001b[34m[2024-07-28 12:41:30.738] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/test\", \"epoch\": 1, \"duration\": 2, \"num_examples\": 2, \"num_bytes\": 14400}\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1722170490.7357342, \"EndTime\": 1722170490.7390378, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"Meta\": \"test_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 200.0, \"count\": 1, \"min\": 200, \"max\": 200}, \"Total Batches Seen\": {\"sum\": 2.0, \"count\": 1, \"min\": 2, \"max\": 2}, \"Max Records Seen Between Resets\": {\"sum\": 200.0, \"count\": 1, \"min\": 200, \"max\": 200}, \"Max Batches Seen Between Resets\": {\"sum\": 2.0, \"count\": 1, \"min\": 2, \"max\": 2}, \"Reset Count\": {\"sum\": 1.0, \"count\": 1, \"min\": 1, \"max\": 1}, \"Number of Records Since Last Reset\": {\"sum\": 200.0, \"count\": 1, \"min\": 200, \"max\": 200}, \"Number of Batches Since Last Reset\": {\"sum\": 2.0, \"count\": 1, \"min\": 2, \"max\": 2}}}\u001b[0m\n",
      "\u001b[34m[07/28/2024 12:41:30 INFO 140663685162816] #test_score (algo-1) : ('binary_classification_accuracy', 1.0)\u001b[0m\n",
      "\u001b[34m[07/28/2024 12:41:30 INFO 140663685162816] #test_score (algo-1) : ('binary_classification_cross_entropy', 0.0)\u001b[0m\n",
      "\u001b[34m[07/28/2024 12:41:30 INFO 140663685162816] #test_score (algo-1) : ('binary_f_1.000', 1.0)\u001b[0m\n",
      "\u001b[34m[07/28/2024 12:41:30 INFO 140663685162816] #quality_metric: host=algo-1, test binary_classification_accuracy <score>=1.0\u001b[0m\n",
      "\u001b[34m[07/28/2024 12:41:30 INFO 140663685162816] #quality_metric: host=algo-1, test binary_classification_cross_entropy <loss>=0.0\u001b[0m\n",
      "\u001b[34m[07/28/2024 12:41:30 INFO 140663685162816] #quality_metric: host=algo-1, test binary_f_1.000 <score>=1.0\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1722170490.734049, \"EndTime\": 1722170490.7396867, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"setuptime\": {\"sum\": 18.001079559326172, \"count\": 1, \"min\": 18.001079559326172, \"max\": 18.001079559326172}, \"totaltime\": {\"sum\": 96.4956283569336, \"count\": 1, \"min\": 96.4956283569336, \"max\": 96.4956283569336}}}\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "2024-07-28 12:41:50 Completed - Training job completed\n",
      "Training seconds: 175\n",
      "Billable seconds: 175\n"
     ]
    }
   ],
   "source": [
    "from sagemaker import get_execution_role\n",
    "from sagemaker.estimator import Estimator\n",
    "\n",
    "# Define the role and S3 paths\n",
    "role = get_execution_role()\n",
    "output_path = f's3://{bucket}/{prefix}/output'\n",
    "\n",
    "# Create and configure the Factorization Machines estimator\n",
    "fm = Estimator(\n",
    "    image_uri=sagemaker.image_uris.retrieve(\"factorization-machines\", boto3.Session().region_name),\n",
    "    role=role,\n",
    "    instance_count=1,\n",
    "    instance_type='ml.m4.xlarge',\n",
    "    output_path=output_path\n",
    ")\n",
    "\n",
    "fm.set_hyperparameters(\n",
    "    feature_dim=train_features.shape[1],\n",
    "    num_factors=10,\n",
    "    predictor_type='binary_classifier',\n",
    "    mini_batch_size=100\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "fm.fit({'train': f's3://{bucket}/{s3_object}', 'test': f's3://{bucket}/{test_s3_object}'})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9311eaec",
   "metadata": {},
   "source": [
    "DEPLOYING THE MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e0048801",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker:Creating model with name: factorization-machines-2024-07-28-12-45-44-346\n",
      "INFO:sagemaker:Creating endpoint-config with name factorization-machines-2024-07-28-12-45-44-346\n",
      "INFO:sagemaker:Creating endpoint with name factorization-machines-2024-07-28-12-45-44-346\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------!"
     ]
    }
   ],
   "source": [
    "#Deploying the model\n",
    "predictor = fm.deploy(initial_instance_count=1, instance_type='ml.m4.xlarge')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dabc9fc",
   "metadata": {},
   "source": [
    "PREPARING TESTING DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7eac50e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import pandas as pd\n",
    "from io import StringIO\n",
    "from sagemaker.predictor import Predictor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Assuming test_features is a numpy array and we need to convert it to CSV format\n",
    "test_features_df = pd.DataFrame(test_features)\n",
    "\n",
    "# Convert the DataFrame to CSV\n",
    "csv_buffer = StringIO()\n",
    "test_features_df.to_csv(csv_buffer, header=False, index=False)\n",
    "\n",
    "# Get the CSV data as a string\n",
    "csv_data = csv_buffer.getvalue()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da736872",
   "metadata": {},
   "source": [
    "TESTING THE MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "661159ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction result: {\"predictions\": [{\"score\": 1.0, \"predicted_label\": 1.0}]}\n"
     ]
    }
   ],
   "source": [
    "import boto3\n",
    "import pandas as pd\n",
    "from io import StringIO\n",
    "import sagemaker.amazon.common as smac\n",
    "import numpy as np\n",
    "\n",
    "# Prepare a single test example\n",
    "single_test_feature = X_test.iloc[0].to_numpy().astype('float32').reshape(1, -1)\n",
    "single_test_label = y_test.iloc[0].astype('float32').reshape(1, -1)\n",
    "\n",
    "# Convert the single test example to RecordIO Protobuf format\n",
    "buf = io.BytesIO()\n",
    "smac.write_numpy_to_dense_tensor(buf, single_test_feature)\n",
    "buf.seek(0)\n",
    "payload = buf.getvalue()\n",
    "\n",
    "# Create a SageMaker runtime client\n",
    "runtime_client = boto3.client('runtime.sagemaker')\n",
    "\n",
    "# Define the endpoint name\n",
    "endpoint_name = 'factorization-machines-2024-07-28-12-45-44-346'\n",
    "\n",
    "# Invoke the endpoint with the correct content type\n",
    "response = runtime_client.invoke_endpoint(\n",
    "    EndpointName=endpoint_name,\n",
    "    ContentType='application/x-recordio-protobuf',\n",
    "    Body=payload\n",
    ")\n",
    "\n",
    "# Parse the response\n",
    "result = response['Body'].read().decode('utf-8')\n",
    "print('Prediction result:', result)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06e85811",
   "metadata": {},
   "source": [
    "MODEL EVALUATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1ba36102",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test RMSE: 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages/sklearn/metrics/_regression.py:492: FutureWarning: 'squared' is deprecated in version 1.4 and will be removed in 1.6. To calculate the root mean squared error, use the function'root_mean_squared_error'.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import boto3\n",
    "import pandas as pd\n",
    "from io import BytesIO\n",
    "import sagemaker.amazon.common as smac\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import numpy as np\n",
    "\n",
    "# Initialize boto3 client\n",
    "s3 = boto3.client('s3')\n",
    "\n",
    "# S3 bucket and file details\n",
    "bucket_name = 'interactionsdataset'\n",
    "file_key = 'synthetic_user_interactions.csv'\n",
    "\n",
    "# Download the file from S3\n",
    "s3_object = s3.get_object(Bucket=bucket_name, Key=file_key)\n",
    "s3_data = s3_object['Body'].read().decode('utf-8')\n",
    "\n",
    "# Load the data into a pandas DataFrame\n",
    "df = pd.read_csv(StringIO(s3_data))\n",
    "\n",
    "# Data preprocessing\n",
    "df['timestamp'] = pd.to_datetime(df['timestamp'])\n",
    "df['userId_encoded'] = df['userId'].astype('category').cat.codes\n",
    "df['attractionId_encoded'] = df['attractionId'].astype('category').cat.codes\n",
    "df['interactionType_view'] = df['interactionType'].apply(lambda x: 1 if x == 'view' else 0)\n",
    "\n",
    "# Feature engineering\n",
    "df['user_interaction_count'] = df.groupby('userId_encoded')['userId_encoded'].transform('count')\n",
    "df['attraction_popularity'] = df.groupby('attractionId_encoded')['attractionId_encoded'].transform('count')\n",
    "max_timestamp = df['timestamp'].max()\n",
    "df['interaction_recency'] = (max_timestamp - df['timestamp']).dt.days\n",
    "df['day_of_week'] = df['timestamp'].dt.dayofweek\n",
    "df['hour_of_day'] = df['timestamp'].dt.hour\n",
    "\n",
    "# Select features for the model\n",
    "features = ['userId_encoded', 'attractionId_encoded', 'user_interaction_count', \n",
    "            'attraction_popularity', 'interaction_recency', 'day_of_week', 'hour_of_day']\n",
    "target = 'interactionType_view'\n",
    "\n",
    "# Prepare training and testing data\n",
    "X = df[features]\n",
    "y = df[target]\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Prepare the test data in RecordIO Protobuf format\n",
    "test_features = X_test.to_numpy().astype('float32')\n",
    "test_labels = y_test.to_numpy().astype('float32').flatten()\n",
    "buf = BytesIO()\n",
    "smac.write_numpy_to_dense_tensor(buf, test_features)\n",
    "buf.seek(0)\n",
    "payload = buf.getvalue()\n",
    "\n",
    "# Create a SageMaker runtime client\n",
    "runtime_client = boto3.client('runtime.sagemaker')\n",
    "\n",
    "# Define the endpoint name\n",
    "endpoint_name = 'factorization-machines-2024-07-28-12-45-44-346'\n",
    "\n",
    "# Invoke the endpoint with the correct content type\n",
    "response = runtime_client.invoke_endpoint(\n",
    "    EndpointName=endpoint_name,\n",
    "    ContentType='application/x-recordio-protobuf',\n",
    "    Body=payload\n",
    ")\n",
    "\n",
    "# Parse the response\n",
    "result = response['Body'].read().decode('utf-8')\n",
    "\n",
    "# Extract predictions from the JSON response\n",
    "import json\n",
    "predictions = json.loads(result)\n",
    "predicted_labels = [pred['predicted_label'] for pred in predictions['predictions']]\n",
    "\n",
    "# Calculate RMSE\n",
    "rmse = mean_squared_error(test_labels, predicted_labels, squared=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7de451bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Labels (first 10): [1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "Predicted Labels (first 10): [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]\n",
      "Test RMSE: 0.0\n",
      "Distribution of test labels: [  0 200]\n",
      "Distribution of predicted labels: [  0 200]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages/sklearn/metrics/_regression.py:492: FutureWarning: 'squared' is deprecated in version 1.4 and will be removed in 1.6. To calculate the root mean squared error, use the function'root_mean_squared_error'.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import boto3\n",
    "import pandas as pd\n",
    "from io import BytesIO\n",
    "import sagemaker.amazon.common as smac\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import numpy as np\n",
    "import json\n",
    "\n",
    "# Initialize boto3 client\n",
    "s3 = boto3.client('s3')\n",
    "\n",
    "# S3 bucket and file details\n",
    "bucket_name = 'interactionsdataset'\n",
    "file_key = 'synthetic_user_interactions.csv'\n",
    "\n",
    "# Download the file from S3\n",
    "s3_object = s3.get_object(Bucket=bucket_name, Key=file_key)\n",
    "s3_data = s3_object['Body'].read().decode('utf-8')\n",
    "\n",
    "# Load the data into a pandas DataFrame\n",
    "df = pd.read_csv(StringIO(s3_data))\n",
    "\n",
    "# Data preprocessing\n",
    "df['timestamp'] = pd.to_datetime(df['timestamp'])\n",
    "df['userId_encoded'] = df['userId'].astype('category').cat.codes\n",
    "df['attractionId_encoded'] = df['attractionId'].astype('category').cat.codes\n",
    "df['interactionType_view'] = df['interactionType'].apply(lambda x: 1 if x == 'view' else 0)\n",
    "\n",
    "# Feature engineering\n",
    "df['user_interaction_count'] = df.groupby('userId_encoded')['userId_encoded'].transform('count')\n",
    "df['attraction_popularity'] = df.groupby('attractionId_encoded')['attractionId_encoded'].transform('count')\n",
    "max_timestamp = df['timestamp'].max()\n",
    "df['interaction_recency'] = (max_timestamp - df['timestamp']).dt.days\n",
    "df['day_of_week'] = df['timestamp'].dt.dayofweek\n",
    "df['hour_of_day'] = df['timestamp'].dt.hour\n",
    "\n",
    "# Select features for the model\n",
    "features = ['userId_encoded', 'attractionId_encoded', 'user_interaction_count', \n",
    "            'attraction_popularity', 'interaction_recency', 'day_of_week', 'hour_of_day']\n",
    "target = 'interactionType_view'\n",
    "\n",
    "# Prepare training and testing data\n",
    "X = df[features]\n",
    "y = df[target]\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Prepare the test data in RecordIO Protobuf format\n",
    "test_features = X_test.to_numpy().astype('float32')\n",
    "test_labels = y_test.to_numpy().astype('float32').flatten()\n",
    "buf = BytesIO()\n",
    "smac.write_numpy_to_dense_tensor(buf, test_features)\n",
    "buf.seek(0)\n",
    "payload = buf.getvalue()\n",
    "\n",
    "# Create a SageMaker runtime client\n",
    "runtime_client = boto3.client('runtime.sagemaker')\n",
    "\n",
    "# Define the endpoint name\n",
    "endpoint_name = 'factorization-machines-2024-07-28-12-45-44-346'\n",
    "\n",
    "# Invoke the endpoint with the correct content type\n",
    "response = runtime_client.invoke_endpoint(\n",
    "    EndpointName=endpoint_name,\n",
    "    ContentType='application/x-recordio-protobuf',\n",
    "    Body=payload\n",
    ")\n",
    "\n",
    "# Parse the response\n",
    "result = response['Body'].read().decode('utf-8')\n",
    "predictions = json.loads(result)\n",
    "predicted_labels = [pred['predicted_label'] for pred in predictions['predictions']]\n",
    "\n",
    "# Print the first 10 test labels and predicted labels for comparison\n",
    "print(\"Test Labels (first 10):\", test_labels[:10])\n",
    "print(\"Predicted Labels (first 10):\", predicted_labels[:10])\n",
    "\n",
    "# Calculate RMSE\n",
    "rmse = mean_squared_error(test_labels, predicted_labels, squared=False)\n",
    "print('Test RMSE:', rmse)\n",
    "\n",
    "# Check the distribution of the target variable in the test set\n",
    "print(\"Distribution of test labels:\", np.bincount(test_labels.astype(int)))\n",
    "print(\"Distribution of predicted labels:\", np.bincount(np.array(predicted_labels).astype(int)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c92f7b65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall distribution of interactionType_view in the dataset:\n",
      "interactionType_view\n",
      "1    1000\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Check the overall distribution of the target variable in the entire dataset\n",
    "print(\"Overall distribution of interactionType_view in the dataset:\")\n",
    "print(df['interactionType_view'].value_counts())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c2a3c5a",
   "metadata": {},
   "source": [
    "GETTING RECOMMENDATIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c8d7a350",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recommendations for user 4b4d5711-5539-44ef-a4fc-5c55f5d8be37: ['Volcanoes NP (Gorilla trekking)' 'Amahoro Stadium' 'BK Arena'\n",
      " 'Underground caves' 'Amashyuza hot springs']\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import boto3\n",
    "from sagemaker.predictor import Predictor\n",
    "import sagemaker.amazon.common as smac\n",
    "from io import BytesIO\n",
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "# Initialize boto3 client\n",
    "s3 = boto3.client('s3')\n",
    "\n",
    "# S3 bucket and file details\n",
    "bucket_name = 'interactionsdataset'\n",
    "file_key = 'synthetic_user_interactions.csv'\n",
    "\n",
    "# Download the file from S3\n",
    "s3_object = s3.get_object(Bucket=bucket_name, Key=file_key)\n",
    "s3_data = s3_object['Body'].read().decode('utf-8')\n",
    "\n",
    "# Load the data into a pandas DataFrame\n",
    "df = pd.read_csv(StringIO(s3_data))\n",
    "\n",
    "# Data preprocessing\n",
    "df['timestamp'] = pd.to_datetime(df['timestamp'])\n",
    "df['userId_encoded'] = df['userId'].astype('category').cat.codes\n",
    "df['attractionId_encoded'] = df['attractionId'].astype('category').cat.codes\n",
    "df['interactionType_view'] = df['interactionType'].apply(lambda x: 1 if x == 'view' else 0)\n",
    "\n",
    "# Feature engineering\n",
    "df['user_interaction_count'] = df.groupby('userId_encoded')['userId_encoded'].transform('count')\n",
    "df['attraction_popularity'] = df.groupby('attractionId_encoded')['attractionId_encoded'].transform('count')\n",
    "max_timestamp = df['timestamp'].max()\n",
    "df['interaction_recency'] = (max_timestamp - df['timestamp']).dt.days\n",
    "df['day_of_week'] = df['timestamp'].dt.dayofweek\n",
    "df['hour_of_day'] = df['timestamp'].dt.hour\n",
    "\n",
    "# Select features for the model\n",
    "features = ['userId_encoded', 'attractionId_encoded', 'user_interaction_count', \n",
    "            'attraction_popularity', 'interaction_recency', 'day_of_week', 'hour_of_day']\n",
    "\n",
    "# Define a function to get recommendations\n",
    "def get_recommendations(user_id, n_recommendations=5):\n",
    "    # Encode the user_id\n",
    "    user_encoded = df.loc[df['userId'] == user_id, 'userId_encoded'].values[0]\n",
    "\n",
    "    # Prepare the data for all attractions for this user\n",
    "    user_data = []\n",
    "    for attraction_id in df['attractionId_encoded'].unique():\n",
    "        interaction_data = [\n",
    "            user_encoded,\n",
    "            attraction_id,\n",
    "            df[df['attractionId_encoded'] == attraction_id]['user_interaction_count'].values[0],\n",
    "            df[df['attractionId_encoded'] == attraction_id]['attraction_popularity'].values[0],\n",
    "            df[df['attractionId_encoded'] == attraction_id]['interaction_recency'].values[0],\n",
    "            df[df['attractionId_encoded'] == attraction_id]['day_of_week'].values[0],\n",
    "            df[df['attractionId_encoded'] == attraction_id]['hour_of_day'].values[0]\n",
    "        ]\n",
    "        user_data.append(interaction_data)\n",
    "\n",
    "    # Convert to numpy array and prepare payload\n",
    "    user_data_np = np.array(user_data).astype('float32')\n",
    "    buf = BytesIO()\n",
    "    smac.write_numpy_to_dense_tensor(buf, user_data_np)\n",
    "    buf.seek(0)\n",
    "    payload = buf.getvalue()\n",
    "\n",
    "    # Create a SageMaker runtime client\n",
    "    runtime_client = boto3.client('runtime.sagemaker')\n",
    "\n",
    "    # Define the endpoint name\n",
    "    endpoint_name = 'factorization-machines-2024-07-28-12-45-44-346'\n",
    "\n",
    "    # Invoke the endpoint with the correct content type\n",
    "    response = runtime_client.invoke_endpoint(\n",
    "        EndpointName=endpoint_name,\n",
    "        ContentType='application/x-recordio-protobuf',\n",
    "        Body=payload\n",
    "    )\n",
    "\n",
    "    # Parse the response\n",
    "    result = response['Body'].read().decode('utf-8')\n",
    "    predictions = json.loads(result)\n",
    "    scores = [pred['score'] for pred in predictions['predictions']]\n",
    "\n",
    "    # Get the top n recommendations\n",
    "    top_indices = np.argsort(scores)[-n_recommendations:][::-1]\n",
    "    top_attractions = df.loc[df['attractionId_encoded'].isin(top_indices), 'attractionId'].unique()\n",
    "\n",
    "    return user_id, top_attractions\n",
    "\n",
    "# Example usage\n",
    "user_id = df['userId'].iloc[0]  # Replace with the actual user ID you want recommendations for\n",
    "user_id, recommendations = get_recommendations(user_id)\n",
    "print(f'Recommendations for user {user_id}:', recommendations)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c1dbc6cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recommendations for user 4b4d5711-5539-44ef-a4fc-5c55f5d8be37:\n",
      "Attraction ID: Nyandungu park\n",
      "Attraction ID: Kigali Memorial Site\n",
      "Attraction ID: Cathedral\n",
      "Attraction ID: Royal balloon\n",
      "Attraction ID: Amashyuza hot springs\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import boto3\n",
    "from sagemaker.predictor import Predictor\n",
    "import sagemaker.amazon.common as smac\n",
    "from io import BytesIO, StringIO\n",
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "# Initialize boto3 client\n",
    "s3 = boto3.client('s3')\n",
    "\n",
    "# S3 bucket and file details\n",
    "bucket_name = 'interactionsdataset'\n",
    "file_key = 'synthetic_user_interactions.csv'\n",
    "\n",
    "# Download the file from S3\n",
    "s3_object = s3.get_object(Bucket=bucket_name, Key=file_key)\n",
    "s3_data = s3_object['Body'].read().decode('utf-8')\n",
    "\n",
    "# Load the data into a pandas DataFrame\n",
    "df = pd.read_csv(StringIO(s3_data))\n",
    "\n",
    "# Data preprocessing\n",
    "df['timestamp'] = pd.to_datetime(df['timestamp'])\n",
    "df['userId_encoded'] = df['userId'].astype('category').cat.codes\n",
    "df['attractionId_encoded'] = df['attractionId'].astype('category').cat.codes\n",
    "df['interactionType_view'] = df['interactionType'].apply(lambda x: 1 if x == 'view' else 0)\n",
    "\n",
    "# Feature engineering\n",
    "df['user_interaction_count'] = df.groupby('userId_encoded')['userId_encoded'].transform('count')\n",
    "df['attraction_popularity'] = df.groupby('attractionId_encoded')['attractionId_encoded'].transform('count')\n",
    "max_timestamp = df['timestamp'].max()\n",
    "df['interaction_recency'] = (max_timestamp - df['timestamp']).dt.days\n",
    "df['day_of_week'] = df['timestamp'].dt.dayofweek\n",
    "df['hour_of_day'] = df['timestamp'].dt.hour\n",
    "\n",
    "# Select features for the model\n",
    "features = ['userId_encoded', 'attractionId_encoded', 'user_interaction_count', \n",
    "            'attraction_popularity', 'interaction_recency', 'day_of_week', 'hour_of_day']\n",
    "\n",
    "# Define a function to get recommendations\n",
    "def get_recommendations(user_id, n_recommendations=5):\n",
    "    # Encode the user_id\n",
    "    user_encoded = df.loc[df['userId'] == user_id, 'userId_encoded'].values[0]\n",
    "\n",
    "    # Prepare the data for all attractions for this user\n",
    "    user_data = []\n",
    "    for attraction_id in df['attractionId_encoded'].unique():\n",
    "        interaction_data = [\n",
    "            user_encoded,\n",
    "            attraction_id,\n",
    "            df[df['attractionId_encoded'] == attraction_id]['user_interaction_count'].values[0],\n",
    "            df[df['attractionId_encoded'] == attraction_id]['attraction_popularity'].values[0],\n",
    "            df[df['attractionId_encoded'] == attraction_id]['interaction_recency'].values[0],\n",
    "            df[df['attractionId_encoded'] == attraction_id]['day_of_week'].values[0],\n",
    "            df[df['attractionId_encoded'] == attraction_id]['hour_of_day'].values[0]\n",
    "        ]\n",
    "        user_data.append(interaction_data)\n",
    "\n",
    "    # Convert to numpy array and prepare payload\n",
    "    user_data_np = np.array(user_data).astype('float32')\n",
    "    buf = BytesIO()\n",
    "    smac.write_numpy_to_dense_tensor(buf, user_data_np)\n",
    "    buf.seek(0)\n",
    "    payload = buf.getvalue()\n",
    "\n",
    "    # Create a SageMaker runtime client\n",
    "    runtime_client = boto3.client('runtime.sagemaker')\n",
    "\n",
    "    # Define the endpoint name\n",
    "    endpoint_name = 'factorization-machines-2024-07-28-12-45-44-346'\n",
    "\n",
    "    # Invoke the endpoint with the correct content type\n",
    "    response = runtime_client.invoke_endpoint(\n",
    "        EndpointName=endpoint_name,\n",
    "        ContentType='application/x-recordio-protobuf',\n",
    "        Body=payload\n",
    "    )\n",
    "\n",
    "    # Parse the response\n",
    "    result = response['Body'].read().decode('utf-8')\n",
    "    predictions = json.loads(result)\n",
    "    scores = [pred['score'] for pred in predictions['predictions']]\n",
    "\n",
    "    # Get the top n recommendations\n",
    "    top_indices = np.argsort(scores)[-n_recommendations:][::-1]\n",
    "    top_attractions = df['attractionId_encoded'].unique()[top_indices]\n",
    "    top_scores = [scores[i] for i in top_indices]\n",
    "\n",
    "    # Map encoded attraction IDs back to original attraction IDs\n",
    "    top_attractions_ids = df.loc[df['attractionId_encoded'].isin(top_attractions), 'attractionId'].unique()\n",
    "    \n",
    "    # Return user_id, top_attractions_ids and top_scores\n",
    "    return user_id, list(zip(top_attractions_ids, top_scores))\n",
    "\n",
    "# Example usage\n",
    "user_id = df['userId'].iloc[0]  # Replace with the actual user ID you want recommendations for\n",
    "user_id, recommendations = get_recommendations(user_id)\n",
    "print(f'Recommendations for user {user_id}:')\n",
    "for attraction, score in recommendations:\n",
    "    print(f'Attraction ID: {attraction}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfb6c084",
   "metadata": {},
   "source": [
    "CHECKING EXISTING ENPOINTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdd589e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "\n",
    "# Create a SageMaker client\n",
    "sagemaker_client = boto3.client('sagemaker')\n",
    "\n",
    "# List endpoints\n",
    "response = sagemaker_client.list_endpoints()\n",
    "\n",
    "# Print the endpoint names\n",
    "for endpoint in response['Endpoints']:\n",
    "    print(endpoint['EndpointName'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57a8c5a5",
   "metadata": {},
   "source": [
    "DELETING ENDPOINTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43f04c3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "\n",
    "# Create a SageMaker client\n",
    "sagemaker_client = boto3.client('sagemaker')\n",
    "\n",
    "# Specify the endpoint name you want to delete\n",
    "endpoint_name = 'factorization-machines-2024-07-25-19-23-18-370'\n",
    "\n",
    "# Delete the endpoint\n",
    "sagemaker_client.delete_endpoint(EndpointName=endpoint_name)\n",
    "\n",
    "print(f\"Endpoint '{endpoint_name}' has been deleted.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
